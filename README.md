# RAG Server - Code-Knowledge Retrieval-Augmented Generation

A production-ready RAG (Retrieval-Augmented Generation) microservice for indexing and querying local codebases with natural language. Built with FastAPI, FAISS, and sentence-transformers.

## Features

- ðŸ” **Hybrid Search**: Combines semantic (vector) and keyword (BM25) search
- ðŸ“ **Multi-Format Support**: Python, PHP, JavaScript, TypeScript, Markdown, and more
- ðŸ¤– **LLM Integration**: Works with OpenAI, Ollama, or retrieval-only mode
- ðŸ“Š **Incremental Indexing**: Only re-index changed files
- ðŸ³ **Docker Ready**: Easy deployment with Docker and docker-compose
- ðŸ” **API Key Authentication**: Secure your endpoints
- ðŸ“ **Source Citations**: All answers include file:line references

## Quick Start

### Local Development

```bash
# Install dependencies
make install

# Run the server
make run

# In another terminal, build the index
make index

# Query the codebase
make query
```

### With Docker

```bash
# Build and run
docker-compose up

# Build index
curl -X POST http://localhost:8000/index/build \
  -H "x-api-key: dev-secret" \
  -H "Content-Type: application/json" \
  -d '{"root":"./","clean":true}'
```

## Configuration

Copy `.env.example` to `.env` and configure:

```bash
# Data directories
RAG_DATA_DIR=./data
RAG_INDEX_DIR=./data/index

# File types to index
RAG_ALLOWED_FILETYPES=.py,.php,.js,.ts,.md,.mdx,.json,.yml,.yaml

# Embedding model
RAG_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# API Security
RAG_API_KEY=dev-secret

# LLM Provider (openai, ollama, or none)
RAG_LLM_PROVIDER=none
OPENAI_API_KEY=your-key-here
OLLAMA_ENDPOINT=http://localhost:11434
LLM_MODEL=gpt-4o-mini
```

## API Endpoints

### Health & Config

```bash
# Health check
curl http://localhost:8000/health

# Get configuration
curl http://localhost:8000/config
```

### Index Management

```bash
# Build index
curl -X POST http://localhost:8000/index/build \
  -H "x-api-key: dev-secret" \
  -H "Content-Type: application/json" \
  -d '{
    "root": "./",
    "clean": true,
    "patterns": ["**/*.py", "**/*.md"],
    "exclude": ["node_modules/**", ".git/**"]
  }'

# Get index stats
curl http://localhost:8000/index/stats \
  -H "x-api-key: dev-secret"

# Incremental update
curl -X POST http://localhost:8000/index/incremental \
  -H "x-api-key: dev-secret" \
  -H "Content-Type: application/json" \
  -d '{"root": "./"}'
```

### Query & Answer

```bash
# Search for relevant code
curl -X POST http://localhost:8000/query \
  -H "x-api-key: dev-secret" \
  -H "Content-Type: application/json" \
  -d '{
    "q": "Where is the file reader implemented?",
    "top_k": 8
  }'

# Generate answer with LLM
curl -X POST http://localhost:8000/answer \
  -H "x-api-key: dev-secret" \
  -H "Content-Type: application/json" \
  -d '{
    "q": "How does the ingestion pipeline work?",
    "top_k": 8,
    "max_tokens": 500
  }'
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Filesystem  â”‚
â”‚ (code+docs) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ingestion        â”‚
â”‚ Pipeline         â”‚
â”‚ - File discovery â”‚
â”‚ - Parsing        â”‚
â”‚ - Chunking       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                  â”‚
       â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector      â”‚   â”‚ Keyword      â”‚
â”‚ Store       â”‚   â”‚ Index        â”‚
â”‚ (FAISS)     â”‚   â”‚ (BM25)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Hybrid        â”‚
        â”‚ Retriever     â”‚
        â”‚ (RRF Fusion)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ LLM           â”‚
        â”‚ (Optional)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ FastAPI       â”‚
        â”‚ Response      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
rag-server/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag_server/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes_admin.py    # Health & config endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ routes_index.py    # Index management
â”‚   â”‚   â”‚   â””â”€â”€ routes_query.py    # Query & answer
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py          # Settings management
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py         # Pydantic models
â”‚   â”‚   â”‚   â””â”€â”€ logging.py         # Structured logging
â”‚   â”‚   â”œâ”€â”€ ingest/
â”‚   â”‚   â”‚   â”œâ”€â”€ readers.py         # File discovery
â”‚   â”‚   â”‚   â”œâ”€â”€ parsers.py         # Code/doc parsing
â”‚   â”‚   â”‚   â”œâ”€â”€ chunking.py        # Text chunking
â”‚   â”‚   â”‚   â””â”€â”€ pipeline.py        # Ingestion orchestration
â”‚   â”‚   â”œâ”€â”€ search/
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_store.py    # FAISS vector search
â”‚   â”‚   â”‚   â”œâ”€â”€ keyword_index.py   # BM25 keyword search
â”‚   â”‚   â”‚   â””â”€â”€ retriever.py       # Hybrid retrieval
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â”œâ”€â”€ openai_client.py   # OpenAI integration
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama_client.py   # Ollama integration
â”‚   â”‚   â”‚   â””â”€â”€ prompt_templates.py# Grounding prompts
â”‚   â”‚   â””â”€â”€ server.py              # FastAPI app
â”‚   â””â”€â”€ main.py                    # Entry point
â”œâ”€â”€ tests/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## Development

```bash
# Install dev dependencies
uv pip install -e ".[dev]"

# Run tests
pytest -v

# Lint code
ruff check .

# Format code
ruff format .

# Type check
mypy src
```

## Example Queries

Try these example queries on your codebase:

- "Where is the database connection configured?"
- "Show me all functions that handle user authentication"
- "How does the caching layer work?"
- "What are the side effects of the `process_data` function?"
- "Explain the API rate limiting implementation"

## Performance

- **Indexing**: > 5 MB/s on SSD
- **Query latency**: < 800ms P95 (warm index)
- **Memory**: < 2 GB for typical codebases
- **Index size**: ~1.5 KB per chunk

## License

MIT License - see LICENSE file

## Contributing

Contributions welcome! Please:
1. Fork the repo
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## Acknowledgments

Built with:
- [FastAPI](https://fastapi.tiangolo.com/) - Modern web framework
- [sentence-transformers](https://www.sbert.net/) - Semantic embeddings
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search
- [rank-bm25](https://github.com/dorianbrown/rank_bm25) - Keyword search
- [structlog](https://www.structlog.org/) - Structured logging
